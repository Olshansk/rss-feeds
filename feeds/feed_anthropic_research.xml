<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Anthropic Research</title>
    <link>https://anthropic.com/research/feed_anthropic_research.xml</link>
    <description>Latest research from Anthropic</description>
    <atom:link href="https://anthropic.com/research/feed_anthropic_research.xml" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <image>
      <url>https://www.anthropic.com/images/icons/apple-touch-icon.png</url>
      <title>Anthropic Research</title>
      <link>https://anthropic.com/research/feed_anthropic_research.xml</link>
    </image>
    <language>en</language>
    <lastBuildDate>Fri, 21 Nov 2025 04:03:42 +0000</lastBuildDate>
    <item>
      <title>Societal Impacts</title>
      <link>https://www.anthropic.com/research/team/societal-impacts</link>
      <description>Societal Impacts</description>
      <guid isPermaLink="false">https://www.anthropic.com/research/team/societal-impacts</guid>
      <category>Research</category>
    </item>
    <item>
      <title>Alignment</title>
      <link>https://www.anthropic.com/research/team/alignment</link>
      <description>Alignment</description>
      <guid isPermaLink="false">https://www.anthropic.com/research/team/alignment</guid>
      <category>Research</category>
    </item>
    <item>
      <title>Interpretability</title>
      <link>https://www.anthropic.com/research/team/interpretability</link>
      <description>Interpretability</description>
      <guid isPermaLink="false">https://www.anthropic.com/research/team/interpretability</guid>
      <category>Research</category>
    </item>
    <item>
      <title>AlignmentDec 18, 2024Alignment faking in large language modelsThis paper provides the first empirical example of a model engaging in alignment faking without being trained to do so—selectively complying with training objectives while strategically preserving existing preferences.</title>
      <link>https://www.anthropic.com/research/alignment-faking</link>
      <description>AlignmentDec 18, 2024Alignment faking in large language modelsThis paper provides the first empirical example of a model engaging in alignment faking without being trained to do so—selectively complying with training objectives while strategically preserving existing preferences.</description>
      <guid isPermaLink="false">https://www.anthropic.com/research/alignment-faking</guid>
      <category>Research</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>AlignmentFeb 3, 2025Constitutional Classifiers: Defending against universal jailbreaksThese classifiers filter the overwhelming majority of jailbreaks while maintaining practical deployment. A prototype withstood over 3,000 hours of red teaming with no universal jailbreak discovered.</title>
      <link>https://www.anthropic.com/research/constitutional-classifiers</link>
      <description>AlignmentFeb 3, 2025Constitutional Classifiers: Defending against universal jailbreaksThese classifiers filter the overwhelming majority of jailbreaks while maintaining practical deployment. A prototype withstood over 3,000 hours of red teaming with no universal jailbreak discovered.</description>
      <guid isPermaLink="false">https://www.anthropic.com/research/constitutional-classifiers</guid>
      <category>Research</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>InterpretabilityMar 27, 2025Tracing the thoughts of a large language modelCircuit tracing lets us watch Claude think, uncovering a shared conceptual space where reasoning happens before being translated into language—suggesting the model can learn something in one language and apply it in another.</title>
      <link>https://www.anthropic.com/research/tracing-thoughts-language-model</link>
      <description>InterpretabilityMar 27, 2025Tracing the thoughts of a large language modelCircuit tracing lets us watch Claude think, uncovering a shared conceptual space where reasoning happens before being translated into language—suggesting the model can learn something in one language and apply it in another.</description>
      <guid isPermaLink="false">https://www.anthropic.com/research/tracing-thoughts-language-model</guid>
      <category>Research</category>
      <pubDate>Thu, 27 Mar 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Sep 15, 2025Economic ResearchAnthropic Economic Index: Tracking AI’s role in the US and global economy</title>
      <link>https://www.anthropic.com/research/economic-index-geography</link>
      <description>Sep 15, 2025Economic ResearchAnthropic Economic Index: Tracking AI’s role in the US and global economy</description>
      <guid isPermaLink="false">https://www.anthropic.com/research/economic-index-geography</guid>
      <category>Research</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Sep 15, 2025Economic ResearchAnthropic Economic Index report: Uneven geographic and enterprise AI adoption</title>
      <link>https://www.anthropic.com/research/anthropic-economic-index-september-2025-report</link>
      <description>Sep 15, 2025Economic ResearchAnthropic Economic Index report: Uneven geographic and enterprise AI adoption</description>
      <guid isPermaLink="false">https://www.anthropic.com/research/anthropic-economic-index-september-2025-report</guid>
      <category>Research</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Oct 3, 2025PolicyBuilding AI for cyber defenders</title>
      <link>https://www.anthropic.com/research/building-ai-cyber-defenders</link>
      <description>Oct 3, 2025PolicyBuilding AI for cyber defenders</description>
      <guid isPermaLink="false">https://www.anthropic.com/research/building-ai-cyber-defenders</guid>
      <category>Research</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Oct 6, 2025AlignmentPetri: An open-source auditing tool to accelerate AI safety research</title>
      <link>https://www.anthropic.com/research/petri-open-source-auditing</link>
      <description>Oct 6, 2025AlignmentPetri: An open-source auditing tool to accelerate AI safety research</description>
      <guid isPermaLink="false">https://www.anthropic.com/research/petri-open-source-auditing</guid>
      <category>Research</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Oct 9, 2025AlignmentA small number of samples can poison LLMs of any size</title>
      <link>https://www.anthropic.com/research/small-samples-poison</link>
      <description>Oct 9, 2025AlignmentA small number of samples can poison LLMs of any size</description>
      <guid isPermaLink="false">https://www.anthropic.com/research/small-samples-poison</guid>
      <category>Research</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Oct 14, 2025PolicyPreparing for AI’s economic impact: exploring policy responses</title>
      <link>https://www.anthropic.com/research/economic-policy-responses</link>
      <description>Oct 14, 2025PolicyPreparing for AI’s economic impact: exploring policy responses</description>
      <guid isPermaLink="false">https://www.anthropic.com/research/economic-policy-responses</guid>
      <category>Research</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>InterpretabilityOct 29, 2025Signs of introspection in large language modelsCan Claude access and report on its own internal states? This research finds evidence for a limited but functional ability to introspect—a step toward understanding what's actually happening inside these models.</title>
      <link>https://www.anthropic.com/research/introspection</link>
      <description>InterpretabilityOct 29, 2025Signs of introspection in large language modelsCan Claude access and report on its own internal states? This research finds evidence for a limited but functional ability to introspect—a step toward understanding what's actually happening inside these models.</description>
      <guid isPermaLink="false">https://www.anthropic.com/research/introspection</guid>
      <category>Research</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Nov 4, 2025AlignmentCommitments on model deprecation and preservation</title>
      <link>https://www.anthropic.com/research/deprecation-commitments</link>
      <description>Nov 4, 2025AlignmentCommitments on model deprecation and preservation</description>
      <guid isPermaLink="false">https://www.anthropic.com/research/deprecation-commitments</guid>
      <category>Research</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Project Fetch: Can Claude train a robot dog?</title>
      <link>https://www.anthropic.com/research/project-fetch-robot-dog</link>
      <description>Project Fetch: Can Claude train a robot dog?</description>
      <guid isPermaLink="false">https://www.anthropic.com/research/project-fetch-robot-dog</guid>
      <category>Research</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>
